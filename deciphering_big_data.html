<!DOCTYPE HTML>
<html>
    <head>
        <title>e-Portfolio by Valentina Mercieca</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="stylesheet" href="assets/css/main.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" /> <!-- Font Awesome -->
        <style>
            /* Remove only the line above the h1 "The Data Professional" */
            .no-line {
                border: none;  /* Ensure no border is applied */
                margin: 0;     /* Remove margin to avoid spacing issues */
                padding: 0;    /* Remove padding */
            }

            /* This is for existing hr elements to keep them as they are */
            hr.major {
                display: block; /* Ensure hr elements show normally */
                margin: 20px 0; /* Adjust spacing as needed */
            }

            /* Increase font size for the paragraph */
            .big-data-paragraph {
                font-size: 1.3em; /* Adjust font size */
                line-height: 1.6; /* Adjust line height for readability */
                margin-bottom: 0.5em; /* Reduced margin between paragraphs */
                text-align: justify; /* Justify text alignment */
            }

            /* Styling for headings */
            h2 {
                font-size: 1.75em;
                margin-top: 1.5em; /* Space before the heading */
                margin-bottom: 1em; /* Space after the heading */
                font-weight: bold;
            }

            h3 {
                font-size: 1.3em;
                margin-top: 1.2em; /* Space before the subheading */
                margin-bottom: 0.5em; /* Space after the subheading */
                font-weight: normal; /* Ensures the text is not bold */
                color: inherit; /* Inherit the color of the surrounding text */
            }


        </style>
    </head>
    <body class="is-preload">

        <!-- Wrapper -->
        <div id="wrapper">

            <!-- Main -->
            <div id="main">
                <div class="inner">

                    <!-- Header -->
                    <header id="header">
                        <a href="index.html" class="logo"><strong>Valentina Mercieca - MSc in Data Science - University of Essex Online</strong></a>
                        <ul class="icons">
                            <li><a href="https://www.linkedin.com/in/valentina-mercieca-473263239" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
                            <li><a href="https://github.com/valentinamercieca" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
                        </ul>
                    </header>


                    <!-- Content -->
                    <section>
                        <header class="main">
                            <h1 class="no-line"> Deciphering Big Data</h1> <!-- Added the class here -->
                        </header>



                        <!-- Increase font size for this paragraph -->
                        <p class="big-data-paragraph">
                            Big data refers to the massive and complex datasets generated by modern systems, offering immense potential for 
                            insights and innovation but also presenting significant challenges in management and analysis.    
                        </p>

                        <p class="big-data-paragraph">
                            Studying this subject is particularly relevant in today's world due to the vast volume of data being generated daily. 
                            According to Pence (2014), current developments indicate that this rate will only increase over time, 
                            making topics like big data management and analysis increasingly critical for the future.
                        </p>
                            
                        <p class="big-data-paragraph">
                            This module provided a comprehensive exploration of big data concepts, equipping me
                            with the tools and techniques necessary to navigate these complexities.
                        </p>

                        <hr class="major" /> <!-- Line here -->



                        <h2>Unit 1: Introduction to Big Data Technologies and Data Management</h2>
                        <h2>Unit 2: Introduction to Data Types and Formats</h2>
                        <h2>Unit 3: Data Collection and Storage</h2>

                        <p class="big-data-paragraph">
                            These three units provided a solid foundation for understanding and managing the complexities 
                            of big data. By examining the 4Vs of big data (Open Sistemas, 2023), I identified challenges and 
                            opportunities in data wrangling, including addressing the vast scale of data (volume), its rapid 
                            generation and processing (velocity), the diversity of formats and structures (variety), and ensuring 
                            quality and reliability (veracity). These considerations underscored the need for robust strategies 
                            to handle diverse formats, maintain data integrity, and implement effective security measures. 
                            The units also covered various data representations—structured, semi-structured, quasi-structured, 
                            and unstructured—and their memory and storage requirements.
                        </p>

                            <p class="big-data-paragraph">
                                Through readings, lectures, and practical tasks, I developed skills to analyse data wrangling problems 
                                and choose appropriate tools and methodologies. Tools such as Python, SQL/MySQL, web services, GUIs, 
                                and APIs enhanced my ability to explore and transform raw data. Practical exercises with <i>NumPy</i>, <i>Pandas</i>, 
                                and <i>Matplotlib</i> helped me systematically pre-process, clean, and manage outliers. 
                                This deepened my understanding of designing automated, standardised data pipelines 
                                to ensure consistency and scalability.
                            </p>
                        
                        
                        <h3 style="font-weight: 600;" class="spaced-heading">Collaborative Discussion 1: The Data Collection Process</h3>

                        <p class="big-data-paragraph">
                            This discussion spanned Units 1 to 3. In this activity, I critically evaluated the opportunities, 
                            limitations, and challenges of large-scale IoT data collection, drawing context from both Huxley (2020) 
                            and Tejada (no date). I highlighted the importance of dual-path architectures (hot and cold paths) 
                            for balancing immediacy and accuracy in IoT systems. Key challenges, such as sensor malfunctions 
                            and security risks, were discussed alongside mitigation strategies like end-to-end encryption and 
                            anomaly detection. This discussion deepened my understanding of data wrangling issues and scalable 
                            storage solutions like data lakes. Engaging with my tutor and peers improved my ability to articulate 
                            technical concepts and respond to feedback constructively.
                        </p>

                        <p class="big-data-paragraph">The discussion posts can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Units%201-3%20Collaborative%20Discussion%201%20-%20The%20Data%20Collection%20Process.pdf" target="_blank">here</a>.</p>

                        
                        <h3 style="font-weight: 600;" class="spaced-heading">Web Scraping Task</h3>
                        <p class="big-data-paragraph">
                            In Unit 3, I created a Python script using <i>requests</i> and <i>BeautifulSoup</i> to 
                            scrape job listings for data scientist roles. The script extracted details such as titles, 
                            companies, and locations while filtering for relevant roles. I addressed challenges 
                            like missing data by implementing checks to ensure robustness, and the results were 
                            stored in a structured JSON file. This task enhanced my practical skills in data extraction 
                            and handling unstructured web data, emphasising the importance of planning and testing in 
                            automated workflows.
                        </p>
                        
                        <p class="big-data-paragraph">The Python file can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%203%20Web%20Scraping.py" target="_blank">here</a>.</p>

                        <style>
                            /* Add space below h3 headings */
                            .spaced-heading {
                                margin-top: 2.5em; /* Adjust the space between the two h3 tags */
                            }
                        </style>


                        <hr class="major" /> <!-- Line here -->

                        <h2>Unit 4: Data Cleaning and Transformation</h2>
                        <p class="big-data-paragraph">
                            Unit 4 explored the challenges and strategies involved in preparing data for analysis 
                            and automation, with an emphasis on practical techniques for improving data quality and 
                            consistency. I examined key considerations in data cleaning, including how to address 
                            missing values, handle inconsistent formats, and meet the requirements of automated 
                            processes.
                        </p>

                        <p class="big-data-paragraph">
                            Using Python, I worked with UNICEF’s survey datasets from Kazil and Jarmul (2016), 
                            which are available on their GitHub repository
                            <a href="https://github.com/jackiekazil/data-wrangling" target="_blank">here</a>. 
                            These practical exercises deepened my understanding of tackling real-world data 
                            issues, particularly in improving data consistency and ensuring standardisation 
                            throughout the process. The exercises continued into Unit 5, where one can find 
                            the Jupyter file available for reference.
                        </p>

                        <h3 style="font-weight: 600;" class="spaced-heading">Data Management Pipeline Test</h3>

                        <p class="big-data-paragraph">
                            This test summarised Python concepts and best practices, including using the CSV writer 
                            object and list generators for data handling. It also covered principles like repository 
                            organisation, clear naming, and version control. The task reinforced my understanding 
                            of Python fundamentals and highlighted the importance of clear, efficient coding practices 
                            aligned with PEP-8 standards.
                        </p>

                        <p class="big-data-paragraph">The test can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%204%20Data%20Management%20Pipeline%20Test.pdf" target="_blank">here</a>.</p>





                        <hr class="major" /> <!-- Line here -->

                        <h2>Unit 5: Data Cleaning and Automating Data Collections</h2>
                        <p class="big-data-paragraph">
                            Building on Unit 4, the data cleaning exercises focused on preparing datasets for analysis. 
                            Key tasks included replacing abbreviated headers with descriptive labels, aligning data using 
                            Python’s zip function, and handling mismatched data. I also utilised <i>strptime</i> and 
                            <i>strftime</i> for datetime manipulation and investigated outliers to ensure data integrity. 
                            Additionally, I explored <i>fuzzy</i> matching techniques using the <i>FuzzyWuzzy</i> library to 
                            identify and resolve inconsistencies in text data, which enhanced my ability to work 
                            with unstructured datasets.
                        </p>

                        <h3 style="font-weight: 600;" class="spaced-heading">Data Cleaning Exercises</h3>

                        <p class="big-data-paragraph">These exercises improved my ability to clean and transform real-world datasets, highlighting Python's versatility and the importance of clear documentation in data preparation.
                        </p>

                        <p class="big-data-paragraph">The Jupyter file can be downloaded <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Units%204-5%20Data%20Cleaning%20Exercises.zip" target="_blank">here</a>.</p>



                        <hr class="major" /> <!-- Line here -->

                        <h2>Unit 6: Database Design and Normalisation</h2>
                        <p class="big-data-paragraph">
                            Unit 6 explored the foundational concepts and methodologies for creating effective databases, 
                            including designing logical structures and translating them into physical architectures for 
                            real-world scenarios. I studied the terminology and processes essential to constructing 
                            relational databases, evaluated design challenges and constraints, and gained a solid 
                            understanding of normalisation principles. This equipped me with the skills to optimise 
                            data storage and retrieval while ensuring data integrity and minimising redundancy.
                        </p>

                        <h3 style="font-weight: 600;" class="spaced-heading">The Development Team Project</h3>
                        <p class="big-data-paragraph">
                            This unit coincided with our team project submission, where I collaborated with two colleagues 
                            over three weeks to design and implement a database for a retail client. My contributions included
                            drafting a team contract, documenting meetings, presenting ideas, and contributing to the 
                            final write-up. Key artefacts included:
                        </p>
                        
                        <ul class="big-data-paragraph" style="text-align: justify;">
                            <li>Team Contract: Defined goals, roles, and procedures to ensure clear communication, accountability, 
                                and effective collaboration.</li>
                            <li>Meeting Notes: Documented key discussions and action points, maintaining a clear 
                                record of progress.</li>
                            <li>Idea Presentation: Proposed a scalable, secure retail database design with 
                                core entities (e.g., Customers, Orders, Products) and relationships.</li>
                            <li>Final Submission: Contributed to the report detailing the logical database design, 
                                data pipeline, and security measures, using Snowflake for analytics and MySQL for backend operations.</li>
                        </ul>
                        
                        <p class="big-data-paragraph">
                            This project sharpened my skills in teamwork, database design, and documentation, 
                            while strengthening my ability to communicate technical concepts and manage project 
                            workflows in a virtual professional setting.
                        </p>
                        
                        <p class="big-data-paragraph">The folder containing the above four components can be found 
                            <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/tree/main/Unit%206%20Development%20Team%20Project" target="_blank">here</a>.
                        </p>
                    



                        <hr class="major" /> <!-- Line here -->

                        <h2>Unit 7: Constructing Normalised Tables and Database Build</h2>
                        <p class="big-data-paragraph">
                            In this unit, I transformed an un-normalised dataset into a fully normalised relational database, 
                            progressing through 1NF, 2NF, and 3NF. This process involved addressing functional dependencies, 
                            eliminating redundancies, and ensuring relational integrity (Kidd, 2024). Step-by-step 
                            normalisation clarified data attributes, associations, and constraints, essential for building an 
                            efficient database. I also developed and tested the relational database in MySQL to ensure 
                            functionality and identify potential issues.
                        </p>

                        <p class="big-data-paragraph">
                            The tasks below enhanced my understanding of normalisation, relational database design, and SQL, 
                            equipping me with key skills for managing structured data.
                        </p>


                        <h3 style="font-weight: 600;" class="spaced-heading">Normalisation Task</h3>
                        <p class="big-data-paragraph">
                            I normalised a table of student and course data by eliminating repeating groups (1NF), 
                            removing partial dependencies (2NF), and addressing transitive dependencies (3NF). 
                            Separate tables were created for students, courses, enrolments, and teachers, with 
                            unique IDs linking data. This improved scalability, consistency, and reduced redundancy.
                        </p>
                        <p class="big-data-paragraph">The normalisation task can be found 
                            <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%207%20Normalisation%20Task.pdf" target=_blank">here.</a></p>

                        <h3 style="font-weight: 600;" class="spaced-heading">Data Build Task</h3>
                        <p class="big-data-paragraph">
                            Using the normalised tables, I created a relational database in SQL, defining primary and foreign 
                            keys to establish relationships. Test data validated the database’s integrity and ensured 
                            accurate query results. This task reinforced my practical skills in database design, 
                            implementation, and testing.
                        </p>
                        <p class="big-data-paragraph">The data build task can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%207%20Data%20Build%20Task.sql" target=_blank">here.</a></p>




                        <hr class="major" /> <!-- Line here -->
                        <h2>Unit 8: Compliance and Regulatory Framework for Managing Data</h2>
                        <h2>Unit 9: Database Management Systems (DBMS) and Models</h2>
                        <h2>Unit 10: Unit 10: More on APIs (Application Programming Interfaces) for Data Parsing</h2>

                        <p class="big-data-paragraph"> 
                            This set of units provided a comprehensive exploration of compliance frameworks, database management 
                            systems (DBMS), and application programming interfaces (APIs), highlighting their interconnected 
                            roles in modern data management.
                        </p>

                        <p class="big-data-paragraph">
                            Unit 8 focused on compliance and regulatory frameworks, particularly GDPR and UK GDPR. I gained insights 
                            into principles like encryption, pseudonymisation, and organisational measures for securing personal data. 
                            The <i>DreamHome Property Management Case Study</i> demonstrated the importance of fact-finding in the database 
                            development lifecycle, highlighting the need to gather terminology, identify system issues, and define 
                            constraints and requirements. Studying this example in chapters 10 and 11 from Connolly and Begg (2015) 
                            helped me appreciate the importance of methodical planning in building structured, effective database 
                            systems.
                        </p>

                        <p class="big-data-paragraph">
                            Unit 9 expanded my knowledge of DBMS, covering relational databases like MySQL and PostgreSQL and 
                            non-relational systems such as MongoDB and Hadoop. I explored their strengths, limitations, 
                            and suitability for various applications, along with advanced technologies like data lakes 
                            and data warehouses for managing large, diverse datasets. These concepts deepened my 
                            understanding of how to design scalable, secure, and efficient database systems.
                        </p>

                        <p class="big-data-paragraph">
                            Unit 10 examined APIs and their role in data parsing and inter-process communication. 
                            I learned about challenges and security requirements for APIs, including encryption, authentication, 
                            and input validation. The Intellas (2016) case study illustrated how APIs integrate distributed 
                            security platforms effectively, underscoring the importance of designing APIs to enhance connectivity 
                            while ensuring security and resilience against vulnerabilities.
                        </p>


                        <h3 style="font-weight: 600;" class="spaced-heading">Collaborative Discussion 2: Comparing Compliance Laws</h3>

                        <p class="big-data-paragraph">
                            Spanning Units 8 to 10, this discussion compared the GDPR and UK GDPR in terms of personal data security. 
                            I explored similarities, including shared requirements for encryption, pseudonymisation, and access 
                            controls, as well as differences in enforcement and exemptions. While the GDPR is uniformly enforced 
                            across the EU, the UK’s ICO takes a risk-based, pragmatic approach, offering detailed guidance for SMEs. 
                            I also highlighted how the ICO balances enforcement with economic considerations, as seen in the reduction 
                            of British Airways' fine after a major data breach (Page, 2020). Furthermore, I examined the broader 
                            exemptions under the UK GDPR, tailored to national priorities, and the resulting compliance challenges 
                            for organisations operating in both regions. This discussion deepened my understanding of the nuanced 
                            application of compliance laws across jurisdictions.
                        </p>

                        <p class="big-data-paragraph">The discussion post can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Units%208-10%20Collaborative%20Discussion%202%20-%20Comparing%20Compliance%20Laws.pdf" target="_blank">here</a>.</p>

                        <h3 style="font-weight: 600;" class="spaced-heading">API Security Requirements Task</h3>

                        <p class="big-data-paragraph">
                            For this artefact, I designed a security requirements specification for a fitness tracker API enabling 
                            data exchange with mobile phones and synchronisation with cloud services or third-party applications. 
                            Key measures included OAuth 2.0 for authentication, HTTPS encryption for secure communication, and 
                            input validation to prevent injection attacks. Format-based security practices ensured robust data 
                            handling across platforms. This task highlighted the importance of secure API design in preventing 
                            vulnerabilities such as DoS attacks and maintaining data integrity.
                        </p>

                        <p class="big-data-paragraph">The task can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%2010%20API%20Security%20Requirements%20Task.pdf" target="_blank">here</a>.</p>
                        

                        <hr class="major" /> <!-- Line here -->
                        <h2>Unit 11: DBMS Transaction and Recovery</h2>
                        <p class="big-data-paragraph"> 
                            In this unit, I explored the importance of maintaining a consistent state within database systems, 
                            ensuring that data remains reliable and intact even during failures. Key topics included transaction 
                            processing, ACID properties (Atomicity, Consistency, Isolation, Durability), and the transaction 
                            manager's role in enforcing these principles. I studied how scheduled transactions, system failures, 
                            and recovery mechanisms like checkpoints preserve database integrity. Learning about commit and 
                            rollback processes provided insights into how systems ensure data consistency during failures 
                            (Paul, 2024). This unit highlighted the critical role of recovery techniques in maintaining data 
                            reliability, particularly in environments with high transaction volumes.
                        </p>

                        <h3 style="font-weight: 600;" class="spaced-heading">Back Up Procedure Task</h3>
                        <p class="big-data-paragraph">
                            For this task, I evaluated the Grandfather-Father-Son (GFS) backup strategy, a hierarchical approach 
                            balancing resource efficiency and recovery needs. By combining daily, weekly, and monthly backups, GFS 
                            reduces storage demands while maintaining essential full backups. Although it lacks real-time recovery 
                            capabilities, it offers a practical and cost-effective solution for structured backup schedules.
                        </p>
                        <p class="big-data-paragraph">The task can be found 
                            <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%2011%20Backup%20Procedure.pdf" target="_blank">here</a>.</p>

                        
                        <h3 style="font-weight: 600;" class="spaced-heading">Individual Project: Executive Summary</h3>
                        <p class="big-data-paragraph">
                            The Unit 11 project expanded on the foundational work from Unit 6, evolving from a logical design phase 
                            to practical implementation. While Unit 6 focused on the creation of a cloud-based database system and 
                            high-level functionality, Unit 11 demonstrated the system’s practicality through SQL implementation, 
                            table creation, and query execution. Security measures were further detailed, including encryption, 
                            audits, and role-based access controls. Additionally, actionable improvements were proposed, such as 
                            optimising ETL processes and integrating a hybrid SQL-NoSQL approach for scalability.
                        </p>
                        <p class="big-data-paragraph">
                            The individual project taught me how to translate theoretical designs into functional systems while 
                            considering business requirements, compliance, and future scalability.
                        </p>

                        <h4 style="font-weight: 600; color: inherit; margin-top: 2em;" >Comparison Table Summary</h4>
                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Unit 6 Initial Proposal</th>
                                    <th>Unit 11 Final Project</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Scope and Objective</td>
                                    <td>Focused on foundational database design.</td>
                                    <td>Implementation and refinement of the design.</td>
                                </tr>
                                <tr>
                                    <td>Technical Depth</td>
                                    <td>Logical design presented with ERD and 3NF.</td>
                                    <td>Practical implementation with SQL code and analysis.</td>
                                </tr>
                                <tr>
                                    <td>Security</td>
                                    <td>General security requirements outlined.</td>
                                    <td>Specific measures proposed with ISO/IEC 27001.</td>
                                </tr>
                                <tr>
                                    <td>Innovations</td>
                                    <td>Recommendations focused on scalability.</td>
                                    <td>Actionable improvements suggested.</td>
                                </tr>
                                <tr>
                                    <td>Analysis</td>
                                    <td>Analysis implied but not demonstrated.</td>
                                    <td>Analytical insights provided through SQL queries.</td>
                                </tr>
                                <tr>
                                    <td>Implementation</td>
                                    <td>Remained in the design phase.</td>
                                    <td>Validated through practical execution.</td>
                                </tr>
                                <tr>
                                    <td>Future-Proofing</td>
                                    <td>Limited future enhancements suggested.</td>
                                    <td>Explored hybrid SQL-NoSQL scalability.</td>
                                </tr>


                            </tbody>
                        </table>



                        <p class="big-data-paragraph"> 
                            The executive summary can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%2011%20Individual%20Project%20Executive%20Summary.pdf" target="_blank">here</a>
                            and the evaluation of the final project versus the initial proposal can be found <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Final%20Project%20(Unit%2011)%20vs%20Initial%20Proposal%20(Unit%206).pdf"target="_blank">here.</a>   
                        </p>


                        <hr class="major" /> <!-- Line here -->
                        <h2>Unit 12: Future of Big Data Analytics</h2>
                        <p class="big-data-paragraph"> 
                            In this final unit, I explored the transformative role of machine learning in advancing big data analytics. 
                            The unit covered emerging trends in database development, focusing on machine learning strategies, 
                            techniques, and algorithms for modelling and analysing large, complex datasets. It also addressed 
                            compliance frameworks and regulatory requirements, emphasising the importance of safeguarding data 
                            privacy and ensuring legal adherence in the evolving landscape of data analytics. This knowledge 
                            provides a foundation for applying these concepts in organisational contexts, enabling businesses to 
                            leverage machine learning for informed decision-making while maintaining compliance.
                        </p>
                        
                        <h3 style="font-weight: 600;" class="spaced-heading">The Reflective Essay</h3>
                        <p class="big-data-paragraph">
                            The reflective essay summarised my learning journey in the module, focusing on skills gained in data 
                            extraction, cleaning, modelling, and teamwork. It highlighted practical experiences such as using 
                            Python for web scraping, normalising datasets, and designing relational databases with SQL. The essay 
                            also reflected on my contributions to the team project.
                        </p>

                        <p class="big-data-paragraph">The essay can be found 
                            <a href="https://github.com/valentinamercieca/Deciphering_Big_Data/blob/main/Unit%2012%20Reflective%20Essay.pdf" target="_blank">here</a>.
                        </p>

                        <hr class="major" /> <!-- Line here -->
                        <h2>References</h2>
                        <p class="big-data-paragraph">

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Connolly, T.M. and Begg, C.E. (2015) Database Systems: A Practical Approach to Design, Implementation and Management. 6th edn. Harlow: Pearson Education Limited. Available at: https://dl.ebooksworld.ir/motoman/Pearson.Database.Systems.A.Practical.Approach.to.Design.Implementation.and.Management.6th.Global.Edition.www.EBooksWorld.ir.pdf (Accessed: 12 December 2024).
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                 Huxley, K. (2020) ‘Data Cleaning’, in Atkinson, P., Delamont, S., Cernat, A., Sakshaug, J.W. and Williams, A. (eds) Quantitative Data Preparation & Secondary Data Analysis. London: SAGE Publications Ltd. Available at: https://doi.org/10.4135/9781526421036842861
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Intellas. (2016) Knowledgebase for Artificial Intelligence Forensics (KAIF) and IBQ QRADAR Security Intelligence Integration Documentation. Available at: https://www.intellas.biz/case-studies/ibm-qradar-kaif-integration (Accessed: 11 December 2024).
                                </span>
                            
                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Kazil, J. and Jarmul, K. (2016) Data Wrangling with Python: Tips and Tools to Make Your Life Easier. 1st edn. Sebastopol: O'Reilly Media Inc.
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Kidd, C. (2024) Data Normalization Explained: An In-Depth Guide. Available at: https://www.splunk.com/en_us/blog/learn/data-normalization.html (Accessed: 26 January 2025).
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Open Sistemas. (2023) The Four V’s of Big Data. Available at: https://opensistemas.com/en/the-four-vs-of-big-data/ (Accessed: 2 January 2025).
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Page, C. (2020) U.K. Privacy Watchdog Hits British Airways With Record-Breaking £20 Million GDPR Fine. Available at: https://www.forbes.com/sites/carlypage/2020/10/16/ico-hits-british-airways-with-record-breaking-fine-for-2018-data-breach/ (Accessed: 13 December 2024).
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Paul, J.I. (2024) Database Transactions Under The Hood: Understanding the Core Processes. Available at: https://cybernerdie.medium.com/database-transactions-under-the-hood-understanding-the-core-processes-6933d898868d (Accessed: 19 January 2025).
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Pence, H.E. (2014) ‘What is Big Data and Why is it Important?’, Journal of Educational Technology Systems, 43(2), pp. 159-171. Available at: https://doi.org/10.2190/ET.43.2.d
                                </span>

                            <span style="display: block; text-align: left; margin-bottom: 15px;">
                                Tejada, Z. (no date) Big data architectures. Available at: https://learn.microsoft.com/en-us/azure/architecture/databases/guide/big-data-architectures (Accessed: 28 October 2024).
                                </span>

                            
                        </p>
                    </section>

                </div>
            </div>
        </div>

        <!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/browser.min.js"></script>
        <script src="assets/js/breakpoints.min.js"></script>
        <script src="assets/js/util.js"></script>
        <script src="assets/js/main.js"></script>

    </body>
</html>
